# -*- coding: utf-8 -*-
"""“Contest_DL_2025.ipynb”的副本

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1W3Fuywsp1PTWOzK2cv2PC6exF1jeVfV3

## Env
"""

# %%capture
import os, re
if "COLAB_" not in "".join(os.environ.keys()):
    !pip install unsloth
else:
    # Do this only in Colab notebooks! Otherwise use pip install unsloth
    import torch
    match = re.match(r"[0-9.]{3,}", str(torch.version))
    if match:
        v = match.group(0)
        xformers = "xformers==" + ("0.0.32.post2" if v == "2.8.0" else "0.0.29.post3")
        !pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo
        !pip install sentencepiece protobuf "datasets>=3.4.1,<4.0.0" "huggingface_hub>=0.34.0" hf_transfer
        !pip install --no-deps unsloth
    else:
        print("Warning: Could not parse torch version. Installing default xformers version.")
        !pip install --no-deps bitsandbytes accelerate xformers peft trl triton cut_cross_entropy unsloth_zoo
        !pip install sentencepiece protobuf "datasets>=3.4.1,<4.0.0" "huggingface_hub>=0.34.0" hf_transfer
        !pip install --no-deps unsloth

!pip install transformers==4.56.2
!pip install --no-deps trl==0.22.2

import os, torch, numpy as np
os.environ.setdefault("BITSANDBYTES_NOWELCOME", "1")

import unsloth

from typing import Dict, Any
from dataclasses import dataclass

from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    TrainingArguments,
    DataCollatorWithPadding,
    BitsAndBytesConfig,
    Trainer,
)
from transformers.trainer_callback import EarlyStoppingCallback

from peft import LoraConfig, get_peft_model, TaskType

print("Torch:", torch.__version__, "CUDA:", torch.version.cuda, "BF16:", torch.cuda.is_bf16_supported())
device = "cuda" if torch.cuda.is_available() else "cpu"
device

"""## Config"""

base_model_name = "unsloth/Meta-Llama-3.1-8B"
max_seq_length = 1024
# 你现有的超参（保持不变），可稍后再按效果调整
lora_cfg = dict(r=16, alpha=32, dropout=0.05)
hp_cfg   = dict(bsz=4, gas=8, lr=2e-4, max_steps=2000)

device = "cuda" if torch.cuda.is_available() else "cpu"
use_bf16 = torch.cuda.is_bf16_supported()

# 是否用 4bit QLoRA
USE_4BIT = True
bnb_config = BitsAndBytesConfig(
    load_in_4bit=USE_4BIT,
    bnb_4bit_compute_dtype=torch.bfloat16 if use_bf16 else torch.float16,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_use_double_quant=True,
) if USE_4BIT else None

"""## Model


"""

tokenizer = AutoTokenizer.from_pretrained(base_model_name, use_fast=True, trust_remote_code=True)
# Llama 系列通常没有 pad_token，设置为 eos，避免报错
if tokenizer.pad_token_id is None:
    tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right"
model = AutoModelForSequenceClassification.from_pretrained(
    base_model_name,
    num_labels=2,
    problem_type="single_label_classification",
    trust_remote_code=True,
    torch_dtype=torch.bfloat16 if use_bf16 else torch.float16,
    device_map="auto" if USE_4BIT else None,
    quantization_config=bnb_config if USE_4BIT else None,
)

# LoRA 目标模块：Llama 常用这组
target_modules = ["q_proj","k_proj","v_proj","o_proj","gate_proj","up_proj","down_proj"]

peft_config = LoraConfig(
    r=lora_cfg["r"],
    lora_alpha=lora_cfg["alpha"],
    lora_dropout=lora_cfg["dropout"],
    bias="none",
    task_type=TaskType.SEQ_CLS,   # 关键：判别式任务
    target_modules=target_modules,
)
model = get_peft_model(model, peft_config)

"""## Data

"""

from datasets import load_dataset

# Load
dataset = load_dataset("ad6398/nyu-dl-teach-maths-comp")
train_ds = dataset["train"]
test_ds  = dataset["test"]

# Create a shuffled small validation split from train
train_full = train_ds.shuffle(seed=42)
val_size = min(2000, int(0.1 * len(train_full)))
val_ds = train_full.select(range(val_size))
train_ds_small = train_full.select(range(val_size, len(train_full)))

print(train_ds_small[0].keys())
print("Train:", len(train_ds_small), "Val:", len(val_ds), "Test:", len(test_ds))

# 统一的输入格式：只作为 Encoder 的输入文本；不再在文本里放标签
INFER_TEMPLATE = (
    "You are a verifier. Decide if the provided solution to the math question is correct.\n"
    "Respond with a binary decision (0 = incorrect, 1 = correct).\n\n"
    "Question:\n{question}\n\n"
    "Solution:\n{solution}\n"
)

def build_input_text(ex: Dict[str, Any]) -> str:
    q = str(ex.get("question", "")).strip()
    s = str(ex.get("solution", "")).strip()
    a = str(ex.get("answer", "")).strip()
    return INFER_TEMPLATE.format(question=q, solution=s)

# Define max_seq_length
 # You might want to adjust this based on your model and data

def preprocess_batch(batch: Dict[str, Any]) -> Dict[str, Any]:
    texts = [build_input_text({"question": q, "solution": s, "answer":a })
             for q, s, a in zip(batch["question"], batch["solution"], batch["answer"])]
    enc = tokenizer(
        texts,
        truncation=True,
        max_length=max_seq_length,
        padding=False,   # 交给 DataCollator 动态 padding，更快
        return_tensors=None,
    )
    # 标签：True->1 / False->0
    labels = [1 if bool(y) else 0 for y in batch["is_correct"]]
    enc["labels"] = labels
    return enc

# 如果你希望仅用部分训练集，可用 select 等；此处直接 map
train_tokenized = train_ds.map(preprocess_batch, batched=True, remove_columns=train_ds.column_names)
val_tokenized   = val_ds.map(preprocess_batch,   batched=True, remove_columns=val_ds.column_names)

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    preds = logits.argmax(axis=-1)
    acc = (preds == labels).mean().item()
    return {"accuracy": acc}

collator = DataCollatorWithPadding(tokenizer=tokenizer, pad_to_multiple_of=8)

"""
### **SFTTrainer Setup**
"""

args = TrainingArguments(
    output_dir=f"outputs/seqcls_lora",
    per_device_train_batch_size=hp_cfg["bsz"],
    per_device_eval_batch_size=hp_cfg["bsz"],
    gradient_accumulation_steps=hp_cfg["gas"],
    learning_rate=hp_cfg["lr"],
    max_steps=hp_cfg["max_steps"],
    warmup_ratio=0.03,                      # 自适应预热（比固定步数稳）
    lr_scheduler_type="cosine",
    optim="adamw_8bit",
    weight_decay=0.01,
    fp16=not use_bf16,
    bf16=use_bf16,
    logging_steps=50,
    report_to="none",
    save_total_limit=2,

    eval_strategy="steps",
    eval_steps=200,
    save_strategy="steps",
    save_steps=200,
    load_best_model_at_end=True,
    metric_for_best_model="accuracy",       # 判别式直接看 acc
    greater_is_better=True,
    seed=42,
)

callbacks = [EarlyStoppingCallback(early_stopping_patience=3, early_stopping_threshold=0.0)]

# =========================
# 10) Trainer & Train
# =========================
trainer = Trainer(
    model=model,
    args=args,
    train_dataset=train_tokenized,
    eval_dataset=val_tokenized,
    data_collator=collator,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
    callbacks=callbacks,
)

trainer.train()

"""
## **Step 6: Inference and Evaluation**

Now that our model is trained, we need to test it on our validation set. We'll use a slightly different prompt for inference—one where we leave the `Output:` section blank for the model to complete.

Let's test it on a single example from our validation set to see what it predicts."""

metrics = trainer.evaluate()
print("Eval metrics:", metrics)

"""## **Step 7: Generate Submission File**

This is the final step\! We will now run our fine-tuned model on the official `test` dataset.

We will loop through each example in the test set, generate a prediction, and format the results into a CSV file with two columns: `ID` and `is_correct`, as required by the competition.

"""

import torch, pandas as pd
from tqdm import tqdm

# 判别式统一模板（和训练时一致）
INFER_TEMPLATE = (
    "You are a verifier. Decide if the provided solution to the math question is correct.\n"
    "Respond with a binary decision (0 = incorrect, 1 = correct).\n\n"
    "Question:\n{question}\n\n"
    "Solution:\n{solution}\n"
)

def build_input_text(q, s):
    return INFER_TEMPLATE.format(question=str(q).strip(), solution=str(s).strip())

predictions = []
for ex in tqdm(test_ds):
    q = ex["question"]
    s = ex.get("solution", "")
    text = build_input_text(q, s)

    enc = tokenizer(text, return_tensors="pt", truncation=True, max_length=max_seq_length).to(model.device)
    with torch.no_grad():
        logits = model(**enc).logits        # [1, 2]
    pred_id = int(logits.argmax(dim=-1).item())   # 0 or 1
    pred_bool = bool(pred_id)                     # 0->False, 1->True
    predictions.append(pred_bool)

submission = pd.DataFrame({"ID": range(len(predictions)), "is_correct": predictions})
submission.to_csv("submission.csv", index=False)
print("Saved submission.csv with", len(predictions), "rows")

"""# SAVE THE MODEL TO DRIVE AND RUN INFERENCE
Add code to save the model checkpoint to Google Drive, load the model from the checkpoint, and generate the final submission CSV file.

## Mount google drive

### Subtask:
Mount Google Drive to save the model checkpoint.

**Reasoning**:
Mount Google Drive to save the model checkpoint.
"""

from google.colab import drive
drive.mount('/content/drive')

"""## Save model checkpoint

### Subtask:
Save the trained model checkpoint to the specified path in Google Drive.

**Reasoning**:
Define the save path and save the model and tokenizer to Google Drive.
"""



import os

# Define the path to save the model checkpoint in Google Drive
save_path = "/content/drive/MyDrive/llama3_8b_math_verifier_checkpoint"

# Create the directory if it doesn't exist
os.makedirs(save_path, exist_ok=True)

# Save the model and tokenizer
model.save_pretrained(save_path)
tokenizer.save_pretrained(save_path)

print(f"Model checkpoint and tokenizer saved to: {save_path}")

"""## Load model from checkpoint

### Subtask:
Load the model from the saved checkpoint.

**Reasoning**:
Load the model and tokenizer from the saved checkpoint path in Google Drive and prepare the model for inference.
"""

# Define the path where the model checkpoint was saved in Google Drive
save_path = "/content/drive/MyDrive/llama3_8b_math_verifier_checkpoint"

# Load the model and tokenizer from the saved path
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = save_path,
    max_seq_length = max_seq_length,
    dtype = dtype,
    load_in_4bit = load_in_4bit,
)

# Prepare the loaded model for faster inference
FastLanguageModel.for_inference(model)

print(f"Model and tokenizer loaded from: {save_path}")

"""## Generate submission file

### Subtask:
Generate the submission CSV file using the loaded model.

**Reasoning**:
Generate the submission CSV file by iterating through the test dataset, generating predictions using the loaded model, and saving the results to a pandas DataFrame.
"""

import pandas as pd
from tqdm import tqdm
from datasets import load_dataset

# Load the official test set
test_dataset = load_dataset("ad6398/nyu-dl-teach-maths-comp", split="test")
predictions = []

# Create the prompt template for inference (no answer included)
inference_prompt = """You are a great mathematician and you are tasked with finding if a solution to a given maths question is correct or not. Your response should be 'True' if the solution is correct, otherwise 'False'. Below is the Question and Solution.
Question:
{}
Solution:
{}
Output:
"""

# A simple function to parse 'True' or 'False' from the model's raw output
def parse_output(response_text):
    # Find the text after "Output:"
    output_part = response_text.split("Output:\n")[-1]
    # Check if "True" is in that part, case-insensitively
    if 'true' in output_part.lower():
        return True
    return False

# Loop through the test dataset and generate a prediction for each example
for example in tqdm(test_dataset):
    question = example["question"]
    solution = example["solution"]

    # Format the prompt
    prompt = inference_prompt.format(question, str(solution))
    inputs = tokenizer([prompt], return_tensors="pt").to("cuda")

    # Generate the prediction
    outputs = model.generate(**inputs, max_new_tokens=8, use_cache=True)
    response_text = tokenizer.batch_decode(outputs)[0]

    # Parse the prediction and add it to our list
    prediction = parse_output(response_text)
    predictions.append(prediction)

# Create the submission DataFrame
submission = pd.DataFrame({
    'ID': range(len(predictions)),
    'is_correct': predictions
})

# Save the DataFrame to a CSV file
submission.to_csv('submission.csv', index=False)

print("\nSubmission file 'submission.csv' created successfully!")
print("You can now download this file and submit it to the Kaggle competition.")